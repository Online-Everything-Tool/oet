name: Validate Generated Tool PR

on:
  pull_request:
    types: [opened, synchronize]
    branches:
      - main # Only run on PRs targeting the main branch

jobs:
  initial_checks:
    name: 1. Initial PR Validations
    if: startsWith(github.head_ref, 'feat/gen-') && github.actor != 'dependabot[bot]'
    runs-on: ubuntu-latest
    permissions:
      contents: read # Read-only for initial checks

    outputs:
      tool_directive: ${{ steps.extract_directive.outputs.tool_directive }}
      pattern_check_passed: ${{ steps.check_patterns.outputs.check_passed }}
      path_validation_passed: ${{ steps.validate_paths.outputs.validation_passed }}
      invalid_files_list: ${{ steps.validate_paths.outputs.invalid_files_list }}
      analysis_succeeded: ${{ steps.analyze_name.outputs.analysis_succeeded }}
      ai_analysis_score: ${{ steps.analyze_name.outputs.score }}
      ai_analysis_is_typo: ${{ steps.analyze_name.outputs.is_typo }}
      ai_analysis_suggestions_json: ${{ steps.analyze_name.outputs.suggestions }}
      ai_analysis_reasoning: ${{ steps.analyze_name.outputs.reasoning }}
      # Combined critical check result for easy downstream use
      critical_initial_checks_passed: ${{ (steps.check_patterns.outputs.check_passed == 'true' && steps.validate_paths.outputs.validation_passed == 'true') }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0 # Fetch all history for accurate diff base

      - name: Extract Tool Directive
        id: extract_directive
        run: |
          BRANCH_NAME="${{ github.head_ref }}"
          TEMP_DIRECTIVE=${BRANCH_NAME#feat/gen-}
          TOOL_DIRECTIVE=$(echo "$TEMP_DIRECTIVE" | sed 's/-[0-9]*$//')
          if [ -z "$TOOL_DIRECTIVE" ]; then 
            echo "::error::Could not extract tool directive from branch name '$BRANCH_NAME'."
            exit 1
          fi
          echo "Extracted Tool Directive: $TOOL_DIRECTIVE"
          echo "tool_directive=${TOOL_DIRECTIVE}" >> $GITHUB_OUTPUT

      - name: Check Directive Pattern
        id: check_patterns
        run: |
          TOOL_DIRECTIVE="${{ steps.extract_directive.outputs.tool_directive }}"
          PATTERN_FILE=".github/tool-directive-patterns.txt"
          MATCH_FOUND=false

          if [ ! -f "$PATTERN_FILE" ]; then 
            echo "::error::Pattern file '$PATTERN_FILE' not found."
            echo "check_passed=false" >> $GITHUB_OUTPUT
            exit 1
          fi

          PATTERNS_TO_CHECK=$(grep -v '^#' "$PATTERN_FILE" | grep -v '^[[:space:]]*$')
          if [ -z "$PATTERNS_TO_CHECK" ]; then 
            echo "::warning::No active patterns found in '$PATTERN_FILE'. Assuming pass for pattern check."
            echo "check_passed=true" >> $GITHUB_OUTPUT
            exit 0
          fi

          echo "$PATTERNS_TO_CHECK" | while IFS= read -r pattern; do
            if echo "$TOOL_DIRECTIVE" | grep -q -E -- "$pattern"; then
              MATCH_FOUND=true; break
            fi
          done

          if $MATCH_FOUND; then 
            echo "Directive '$TOOL_DIRECTIVE' matches an allowed pattern."
            echo "check_passed=true" >> $GITHUB_OUTPUT
          else 
            echo "::error::Directive '$TOOL_DIRECTIVE' does not match any allowed patterns in '$PATTERN_FILE'."
            echo "check_passed=false" >> $GITHUB_OUTPUT
            exit 1 
          fi

      - name: Analyze Directive Name (AI Check)
        id: analyze_name
        if: steps.check_patterns.outputs.check_passed == 'true'
        continue-on-error: true
        env:
          APP_URL: ${{ secrets.APP_URL || 'https://online-everything-tool.com' }}
        run: |
          TOOL_DIRECTIVE="${{ steps.extract_directive.outputs.tool_directive }}"
          API_ENDPOINT="${APP_URL}/api/analyze-directive-name/"
          EXISTING_DIRECTIVES_JSON='[]' 
          GENERATIVE_DESC_TEXT="Function related to ${TOOL_DIRECTIVE//-/ }"

          echo "Attempting AI Analysis API call to: $API_ENDPOINT for directive: $TOOL_DIRECTIVE"
          HTTP_STATUS=$(curl -s -w "%{http_code}" -X POST "$API_ENDPOINT" \
            -H "Content-Type: application/json" \
            -d "{
                  \"proposedDirective\": \"$TOOL_DIRECTIVE\",
                  \"existingDirectives\": $EXISTING_DIRECTIVES_JSON,
                  \"generativeDescription\": \"$GENERATIVE_DESC_TEXT\"
                }" \
            -o response.json)

          echo "AI Analysis API HTTP Status: $HTTP_STATUS"
          # echo "AI Analysis API Response Body:" # Can be verbose
          # cat response.json || echo "<empty response body>"

          ANALYSIS_SUCCEEDED_FLAG="false"; SCORE_VAL="N/A"; IS_TYPO_VAL="N/A"; SUGGESTIONS_JSON_STR="[]"; REASONING_TEXT="Analysis could not be performed."

          if [[ "$HTTP_STATUS" -ge 200 && "$HTTP_STATUS" -lt 300 ]] && jq -e . response.json > /dev/null 2>&1; then
             echo "AI Analysis API call successful and response is valid JSON."
             SCORE_VAL=$(jq -r '.score // 0.5' response.json)
             IS_TYPO_VAL=$(jq -r '.is_likely_typo // false' response.json)
             SUGGESTIONS_JSON_STR=$(jq -c '.suggestions // []' response.json) 
             REASONING_TEXT=$(jq -r '.reasoning // "Analysis incomplete."' response.json)
             ANALYSIS_SUCCEEDED_FLAG="true"
          else
             echo "::warning::AI Analysis API call failed or returned invalid JSON (HTTP Status: $HTTP_STATUS). Analysis results will be marked as N/A."
          fi

          echo "analysis_succeeded=$ANALYSIS_SUCCEEDED_FLAG" >> $GITHUB_OUTPUT
          echo "score=$SCORE_VAL" >> $GITHUB_OUTPUT
          echo "is_typo=$IS_TYPO_VAL" >> $GITHUB_OUTPUT
          echo "suggestions=$SUGGESTIONS_JSON_STR" >> $GITHUB_OUTPUT
          # Multiline output for reasoning
          {
            echo "reasoning<<EOF_REASONING"
            echo "$REASONING_TEXT"
            echo "EOF_REASONING"
          } >> $GITHUB_OUTPUT

      - name: Validate Changed File Paths
        id: validate_paths
        if: steps.check_patterns.outputs.check_passed == 'true'
        env:
          TOOL_DIRECTIVE: ${{ steps.extract_directive.outputs.tool_directive }}
        run: |
          if [ -z "$TOOL_DIRECTIVE" ]; then 
            echo "::error::TOOL_DIRECTIVE not set for path validation."
            echo "validation_passed=false" >> $GITHUB_OUTPUT
            exit 1
          fi

          git fetch origin main --depth=1 # Fetch only the main branch history needed
          BASE_SHA=$(git merge-base HEAD origin/main)
          if [ -z "$BASE_SHA" ]; then 
            echo "::error::Could not determine merge base with main branch."
            echo "validation_passed=false" >> $GITHUB_OUTPUT
            exit 1
          fi

          CHANGED_FILES_WITH_STATUS=$(git diff --name-status $BASE_SHA HEAD)
          if [ -z "$CHANGED_FILES_WITH_STATUS" ]; then 
            echo "No files changed compared to main. Assuming pass for path validation."
            echo "validation_passed=true" >> $GITHUB_OUTPUT
            exit 0
          fi

          ALLOWED_FOLDER_PATTERN="^app/tool/${TOOL_DIRECTIVE}/"
          INVALID_FILES_FOUND=""
          ALL_PATHS_VALID=true

          echo "Validating changed file paths against pattern: $ALLOWED_FOLDER_PATTERN"
          echo "Changed files (Status Path):"
          echo "$CHANGED_FILES_WITH_STATUS"

          echo "$CHANGED_FILES_WITH_STATUS" | while IFS=$'\t' read -r status filepath; do
            if [ -z "$filepath" ]; then continue; fi
            if [[ "$status" == "A" || "$status" == "M" || "$status" == "C" || "$status" == "R" ]]; then # Check Added, Modified, Copied, Renamed
              # For Renamed files, filepath is the new path. The old path would be in the second column if using --name-status R<score> <old> <new>
              # For simplicity, just checking the new path for R.
              actual_path_to_check=$filepath
              if [[ "$status" == "R"* ]]; then # If Renamed, diff output might be R<score>\t<oldpath>\t<newpath>
                  # This simple IFS might not split R<score> old new correctly.
                  # A more robust way for renames would be `git diff --name-only --diff-filter=R $BASE_SHA HEAD` for new paths
                  # and `git diff --name-only --diff-filter=R --find-renames $BASE_SHA HEAD` for old paths.
                  # For now, assume filepath is the relevant path (new path for R).
                  # This part might need refinement if you handle complex renames where old path matters.
                  # For now, we assume $filepath is the path that needs to be in the allowed folder.
                  echo "Rename detected, checking path: $filepath"
              fi

              if [[ ! "$actual_path_to_check" =~ $ALLOWED_FOLDER_PATTERN ]]; then
                echo "::error file=$actual_path_to_check::Invalid path. File status: $status. Must be within '$ALLOWED_FOLDER_PATTERN'."
                INVALID_FILES_FOUND="${INVALID_FILES_FOUND}${actual_path_to_check} (Status: ${status})\n"
                ALL_PATHS_VALID=false
              else
                echo "Valid path: $actual_path_to_check (Status: $status)"
              fi
            else
              echo "Skipping strict path validation for file with status '$status': $filepath"
            fi
          done

          if $ALL_PATHS_VALID; then
            echo "validation_passed=true" >> $GITHUB_OUTPUT
          else
            echo "validation_passed=false" >> $GITHUB_OUTPUT
            # Prepare for multiline output in GitHub Actions
            INVALID_FILES_ESCAPED="${INVALID_FILES_FOUND//'%'/'%25'}"
            INVALID_FILES_ESCAPED="${INVALID_FILES_ESCAPED//$'\n'/'%0A'}"
            INVALID_FILES_ESCAPED="${INVALID_FILES_ESCAPED//$'\r'/'%0D'}"
            echo "invalid_files_list=${INVALID_FILES_ESCAPED}" >> $GITHUB_OUTPUT
            exit 1
          fi

  build_and_run_douglas_checker:
    name: 2. Build Tool & Run Douglas Ethos Check
    needs: initial_checks
    if: success() && needs.initial_checks.outputs.critical_initial_checks_passed == 'true' && github.actor != 'dependabot[bot]'
    runs-on: ubuntu-latest
    permissions:
      contents: read

    outputs:
      douglas_check_step_outcome: ${{ steps.douglas_checker_run.outcome }} # 'success', 'failure', or 'skipped' (if step itself had issues)

    steps:
      - name: Checkout OET Code (PR branch)
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: '**/package-lock.json'

      - name: Install OET Dependencies
        run: npm ci

      - name: Prepare for Static Build (Isolate New Tool)
        env:
          TOOL_DIRECTIVE: ${{ needs.initial_checks.outputs.tool_directive }}
        run: |
          echo "Isolating tool: $TOOL_DIRECTIVE for static check"
          if [ -z "$TOOL_DIRECTIVE" ]; then
            echo "::error::Tool directive not received from initial_checks job. Cannot prune."
            exit 1
          fi
          echo "Removing app/api..."
          rm -rf app/api || echo "No app/api directory to remove or removal failed (continuing)."

          if [ -d "app/tool" ]; then
            echo "Removing other tool directories from app/tool/ ..."
            cd app/tool
            # Ensure TOOL_DIRECTIVE is not empty before using it in grep
            ls -d */ 2>/dev/null | grep -v -E "^(${TOOL_DIRECTIVE}/|_components/|_hooks/)$" | xargs -r rm -rf
            cd ../.. 
          else
            echo "app/tool directory not found, skipping tool pruning."
          fi
          echo "Project structure pruned for static check."

      - name: Build OET for Static Export
        id: build_oet # Give this step an id to check its outcome
        env:
          NEXT_OUTPUT_MODE: export
        run: |
          echo "Attempting static build with NEXT_OUTPUT_MODE=${NEXT_OUTPUT_MODE}..."
          npm run build 
          if [ ! -d "out" ]; then
            echo "::error::Static export 'out' directory not found after build."
            echo "Ensure 'npm run build' respects NEXT_OUTPUT_MODE for 'output: \"export\"'."
            exit 1
          fi
          echo "'out' directory found."
        continue-on-error: true # Allow build to fail but capture outcome

      - name: Serve Static Output Locally
        id: serve_static
        if: steps.build_oet.outcome == 'success' # Only serve if build succeeded
        run: |
          echo "Starting static server for 'out' directory on port 3001..."
          # Ensure port is free
          if sudo fuser 3001/tcp > /dev/null 2>&1; then
            echo "Port 3001 is in use. Attempting to kill process..."
            sudo fuser -k 3001/tcp || echo "Failed to kill process on port 3001, or it was already free."
            sleep 2 
          fi
          npx serve out -l 3001 & 
          SERVER_PID=$!
          echo "Server PID: $SERVER_PID"
          echo "server_pid=$SERVER_PID" >> $GITHUB_OUTPUT
          echo "Local server URL: http://localhost:3001"

          echo "Waiting for server on port 3001..."
          timeout 30s bash -c 'until curl -sSf http://localhost:3001 > /dev/null; do echo -n "."; sleep 1; done' \
          || (echo "::error::Local server (npx serve) did not start on port 3001 in time." && (sudo kill -9 $SERVER_PID || true) && exit 1)
          echo "Server is up!"
        continue-on-error: true # Allow server step to fail but capture outcome

      - name: Clone Douglas Checker
        if: steps.serve_static.outcome == 'success' # Only if server started
        run: |
          echo "Cloning Douglas checker..."
          git clone https://github.com/Online-Everything-Tool/douglas.git ./douglas-checker
          if [ ! -d "./douglas-checker" ]; then
            echo "::error::Failed to clone Douglas checker repository."
            exit 1
          fi

      - name: Install Douglas Checker Dependencies
        if: steps.serve_static.outcome == 'success'
        working-directory: ./douglas-checker
        run: |
          echo "Installing Douglas dependencies..."
          if [ -f "package-lock.json" ]; then
            npm ci
          else
            npm install
          fi

      - name: Run Douglas Ethos Check
        id: douglas_checker_run
        if: steps.serve_static.outcome == 'success' # Only if server started
        working-directory: ./douglas-checker
        env:
          TOOL_DIRECTIVE: ${{ needs.initial_checks.outputs.tool_directive }}
        run: |
          echo "Running Douglas check for tool: $TOOL_DIRECTIVE"
          TARGET_URL="http://localhost:3001/tool/$TOOL_DIRECTIVE/"
          SUMMARY_FILE_PATH="$RUNNER_TEMP/douglas_summary.md"
          SCREENSHOT_FILE_PATH="$RUNNER_TEMP/douglas_screenshot.png"

          if [ ! -f "./dist/check-tool.js" ]; then
            echo "Douglas checker not compiled. Attempting to compile..."
            npx tsc || (echo "::error::Failed to compile Douglas checker." && exit 1)
            if [ ! -f "./dist/check-tool.js" ]; then
              echo "::error::Douglas checker (dist/check-tool.js) still not found after compile attempt." && exit 1
            fi
          fi

          COMMAND_ARGS=("$TARGET_URL" --outputSummaryFile "$SUMMARY_FILE_PATH" --screenshotPath "$SCREENSHOT_FILE_PATH")
          echo "Executing: node ./dist/check-tool.js ${COMMAND_ARGS[@]}"
          node ./dist/check-tool.js "${COMMAND_ARGS[@]}"
        continue-on-error: true

      - name: Upload Douglas Summary Artifact
        if: always() && steps.serve_static.outcome == 'success' # Only if server was attempted
        uses: actions/upload-artifact@v4
        with:
          name: douglas-summary-${{ github.run_id }}
          path: ${{ runner.temp }}/douglas_summary.md
          if-no-files-found: warn

      - name: Upload Douglas Screenshot Artifact
        if: always() && steps.serve_static.outcome == 'success'
        uses: actions/upload-artifact@v4
        with:
          name: douglas-screenshot-${{ github.run_id }}
          path: ${{ runner.temp }}/douglas_screenshot.png
          if-no-files-found: warn

      - name: Kill Static Server
        if: always() && steps.serve_static.outcome == 'success' && steps.serve_static.outputs.server_pid
        run: |
          echo "Attempting to kill static server PID: ${{ steps.serve_static.outputs.server_pid }}..."
          (sudo kill -9 ${{ steps.serve_static.outputs.server_pid }} || echo "Kill failed, server might be already stopped.")
          # Double check with fuser
          if sudo fuser 3001/tcp > /dev/null 2>&1; then
             echo "Port 3001 still in use after PID kill, attempting fuser kill..."
             sudo fuser -k 3001/tcp || echo "fuser kill also failed or port now free."
          else
             echo "Port 3001 is free."
          fi

  report_pr_status:
    name: 3. Report PR Validation Status
    needs:
      - initial_checks
      - build_and_run_douglas_checker
    if: always() && github.actor != 'dependabot[bot]'
    runs-on: ubuntu-latest
    permissions:
      pull-requests: write
      actions: read

    steps:
      - name: Download Douglas Summary Artifact
        id: download_summary
        uses: actions/download-artifact@v4
        with:
          name: douglas-summary-${{ github.run_id }}
          path: ${{ runner.temp }}/douglas-artifacts/summary
        continue-on-error: true

      - name: Read Douglas Summary Content
        id: read_summary
        # Only run if download was successful AND the file actually exists
        if: steps.download_summary.outcome == 'success' && (join(runner.os, '') != 'Windows' && system('test -f ${{ runner.temp }}/douglas-artifacts/summary/douglas_summary.md') == 0 || join(runner.os, '') == 'Windows' && system('Test-Path ${{ runner.temp }}/douglas-artifacts/summary/douglas_summary.md') == 'True')
        run: |
          SUMMARY_FILE="${{ runner.temp }}/douglas-artifacts/summary/douglas_summary.md"
          SUMMARY_CONTENT=$(cat "$SUMMARY_FILE")
          # Escape for multiline environment variable and then for JSON in github-script
          SUMMARY_CONTENT="${SUMMARY_CONTENT//'%'/'%25'}" # Must be first for %0A, %0D
          SUMMARY_CONTENT="${SUMMARY_CONTENT//$'\n'/'%0A'}"
          SUMMARY_CONTENT="${SUMMARY_CONTENT//$'\r'/'%0D'}"
          echo "summary_markdown=${SUMMARY_CONTENT}" >> $GITHUB_OUTPUT
        shell: bash
        continue-on-error: true # If cat fails, use default in env

      - name: Construct and Post PR Comment
        id: post_comment
        uses: actions/github-script@v7
        env:
          PR_NUMBER: ${{ github.event.pull_request.number }}
          TOOL_DIRECTIVE: ${{ needs.initial_checks.outputs.tool_directive }}
          INITIAL_CHECKS_JOB_RESULT: ${{ needs.initial_checks.result }}
          PATTERN_CHECK_PASSED: ${{ needs.initial_checks.outputs.pattern_check_passed }}
          PATH_VALIDATION_PASSED: ${{ needs.initial_checks.outputs.path_validation_passed }}
          INVALID_FILES_LIST: ${{ needs.initial_checks.outputs.invalid_files_list }}
          AI_ANALYSIS_SUCCEEDED: ${{ needs.initial_checks.outputs.analysis_succeeded }}
          AI_SCORE: ${{ needs.initial_checks.outputs.ai_analysis_score }}
          AI_IS_TYPO: ${{ needs.initial_checks.outputs.ai_analysis_is_typo }}
          AI_SUGGESTIONS_JSON: ${{ needs.initial_checks.outputs.ai_analysis_suggestions_json }}
          AI_REASONING: ${{ needs.initial_checks.outputs.ai_analysis_reasoning }}

          BUILD_DOUGLAS_JOB_RESULT: ${{ needs.build_and_run_douglas_checker.result }}
          DOUGLAS_CHECK_STEP_OUTCOME: ${{ needs.build_and_run_douglas_checker.outputs.douglas_check_step_outcome || 'skipped' }}
          DOUGLAS_SUMMARY_MD: ${{ steps.read_summary.outputs.summary_markdown || 'Douglas summary artifact not found, unreadable, or check was skipped.' }}
          ACTION_RUN_URL: 'https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}'

        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const { repo, owner } = context.repo;
            const issue_number = parseInt(process.env.PR_NUMBER, 10);
            if (isNaN(issue_number)) {
              core.setFailed("PR Number is not available for commenting.");
              return;
            }

            const toolDirective = process.env.TOOL_DIRECTIVE || "UnknownTool";
            let commentBody = `## 🤖 OET Tool PR Validation Status for \`${toolDirective}\`\n\n`;

            // --- 1. Initial Checks ---
            commentBody += `### 1. Initial PR Validations\n`;
            const initialChecksJobResult = process.env.INITIAL_CHECKS_JOB_RESULT;
            const patternPassed = process.env.PATTERN_CHECK_PASSED === 'true';
            const pathsPassed = process.env.PATH_VALIDATION_PASSED === 'true';

            if (initialChecksJobResult === 'success') {
              commentBody += `✅ **Initial Validations Passed!**\n`;
              commentBody += `  - Directive \`${toolDirective}\` matches allowed patterns.\n`;
              commentBody += `  - All changed files are within the allowed directory.\n`;
            } else if (initialChecksJobResult === 'failure') {
              commentBody += `🚨 **Initial Validations FAILED!**\n`;
              if (!patternPassed) commentBody += `  - **Pattern Check Failed:** Directive \`${toolDirective}\` does not match.\n`;
              if (!pathsPassed) {
                const invalidFiles = (process.env.INVALID_FILES_LIST || "List unavailable.").replace(/%0A/g, '\n    - ').replace(/%0D/g, '');
                commentBody += `  - **Path Validation Failed:** Files changed outside allowed directory. Invalid files:\n    - ${invalidFiles}\n`;
              }
            } else {
               commentBody += `🟡 Initial validations were skipped or had an issue (${initialChecksJobResult}).\n`;
            }

            commentBody += `\n  **🤖 AI Directive Name Analysis:**\n`;
            if (process.env.AI_ANALYSIS_SUCCEEDED === 'true') {
              commentBody += `    - Score: ${process.env.AI_SCORE}\n`;
              commentBody += `    - Likely Typo: ${process.env.AI_IS_TYPO === 'true' ? 'Yes ❗' : 'No'}\n`;
              let reasoning = process.env.AI_REASONING || "No reasoning provided.";
              // Ensure reasoning is treated as a single block of text in Markdown
              reasoning = reasoning.replace(/%0A/g, '\n    '); // Indent subsequent lines
              commentBody += `    - Reasoning:\n    ${reasoning}\n`;
              try {
                const suggestions = JSON.parse(process.env.AI_SUGGESTIONS_JSON || '[]');
                if (suggestions.length > 0) {
                  commentBody += `    - Suggestions: ${suggestions.map(s => `\`${s}\``).join(', ')}\n`;
                }
              } catch (e) { console.warn("Could not parse AI suggestions JSON for comment"); }
            } else {
              commentBody += `    - ⚠️ AI name analysis could not be performed or API call failed.\n`;
            }
            commentBody += "\n---\n";

            // --- 2. Local Build & Douglas Ethos Check ---
            commentBody += `### 2. Local Build & Douglas Ethos Check\n`;
            const buildDouglasJobResult = process.env.BUILD_DOUGLAS_JOB_RESULT;
            const buildStepSucceeded = (buildDouglasJobResult === 'success'); // Simplistic check, outcome of job matters more

            if (buildDouglasJobResult === 'skipped') {
              commentBody += `🟡 Local build and Douglas check were skipped (likely due to initial validation failures).\n`;
            } else {
              // Check if the build step itself (within the job) succeeded or failed
              // This requires an output from the build step, or inferring from job result
              // For now, we assume if job isn't skipped, build was attempted.
              // A more robust check would be an explicit output from the build step.
              // We use buildDouglasJobResult to infer build success or failure.
              if(buildStepSucceeded) {
                commentBody += `✅ **Local Static Build Successful.**\n`;
              } else {
                commentBody += `🚨 **Local Static Build FAILED!** (or Douglas job setup failed before Douglas script ran)\n`;
                commentBody += `   Please check the [Action run logs](${process.env.ACTION_RUN_URL}?pr=${issue_number}#step:2:1) for details on the build failure.\n`;
              }

              // Douglas summary content
              const douglasSummary = process.env.DOUGLAS_SUMMARY_MD.replace(/%0A/g, '\n').replace(/%0D/g, '');
              commentBody += `\n${douglasSummary}\n`; // Douglas summary has its own header
              if (buildDouglasJobResult !== 'skipped') {
                commentBody += `   (View full Douglas artifacts including screenshot [on the Action run page](${process.env.ACTION_RUN_URL}?pr=${issue_number}))\n`;
              }
            }
            commentBody += "\n---\n";

            // --- 3. Next Steps ---
            commentBody += `### 3. Next Steps\n`;
            const criticalInitialChecksPassed = initialChecksJobResult === 'success';
            const douglasActualCheckPassed = process.env.DOUGLAS_CHECK_STEP_OUTCOME === 'success';
            // Assuming build success if Douglas check was even attempted and passed/failed based on its own logic
            const overallBuildAndDouglasSuccess = buildDouglasJobResult === 'success' && douglasActualCheckPassed;


            if (criticalInitialChecksPassed && overallBuildAndDouglasSuccess) {
              commentBody += `✅ All local checks passed!\n`;
              commentBody += `⏳ A Netlify Deploy Preview should be building now. Its status will appear as a separate check from Netlify on this PR.\n`;
              commentBody += `👍 Once ready, please test your tool thoroughly using the Deploy Preview link!\n`;
            } else {
              commentBody += `‼️ **Action Required:** One or more critical local checks failed. Please review the details above and in the [Action run logs](${process.env.ACTION_RUN_URL}?pr=${issue_number}).\n`;
              commentBody += `🔴 The Netlify Deploy Preview may not build successfully or may not reflect the intended functionality until these issues are addressed.\n`;
            }

            // Post the comment
            try {
              await github.rest.issues.createComment({ owner, repo, issue_number, body: commentBody });
            } catch (e) {
              core.error(`Failed to post PR comment: ${e.message}`);
            }

            // --- Determine overall workflow failure ---
            let failWorkflow = false;
            let failureReason = "";

            if (initialChecksJobResult === 'failure') {
              failWorkflow = true;
              failureReason += "Initial PR Validations failed. ";
            }
            // If build_and_run_douglas_checker job failed, it's a critical failure.
            // This could be build failure, server start failure, or Douglas finding issues (if Douglas script failure propagates to job failure)
            if (buildDouglasJobResult === 'failure') {
              failWorkflow = true;
              failureReason += "Local Build or Douglas Ethos Check process failed. ";
            } else if (buildDouglasJobResult === 'success' && !douglasActualCheckPassed) {
              // If build job succeeded, but the Douglas step within it specifically failed
              failWorkflow = true;
              failureReason += "Douglas Ethos Check detected violations. ";
            }

            if (failWorkflow) {
              core.setFailed(`Workflow failed: ${failureReason.trim()} See PR comment and Action logs for details.`);
            }
      - name: Fail workflow if checks failed (explicit)
        if: steps.post_comment.outputs.outcome == 'success' && (needs.initial_checks.result == 'failure' || needs.build_and_run_douglas_checker.result == 'failure' || (needs.build_and_run_douglas_checker.result == 'success' && needs.build_and_run_douglas_checker.outputs.douglas_check_step_outcome != 'success'))
        run: |
          echo "One or more critical checks failed. Marking workflow as failed."
          exit 1
